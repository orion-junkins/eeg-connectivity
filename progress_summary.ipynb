{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progress Summary\n",
    "This document checks what processing steps have been completed for each subject and gives some utilities for generating bash scripts to run needed pipelines.\n",
    "\n",
    "It should be regarded as a rough draft/work in progress to be tweaked for a particular need, but is a good starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "ICA_COMPS = 0.9999\n",
    "\n",
    "root = \"/Volumes/eeg\"\n",
    "connectivity_dir = \"connectivity_scores_shannon_entropy_dur2_ovlp1\"\n",
    "\n",
    "def process(df, raw_dir, preprocessed_dir, processed_dir, ica_dir, connectivity_dir, notes_dir):\n",
    "    fdt_set_exists = []\n",
    "    raw_exists = []\n",
    "    preprocessed_exists = []\n",
    "    bads_file_exists = []\n",
    "    processed_exists = []\n",
    "    ica_exists = []\n",
    "    ica_drops = []\n",
    "    connectivity_exists = []\n",
    "    notes = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        id = row[\"id\"]\n",
    "        session = row[\"session\"]\n",
    "        fdt_path = os.path.join(raw_dir, id, f'{session}.fdt')\n",
    "        set_path = os.path.join(raw_dir, id, f'{session}.set')\n",
    "        raw_path = os.path.join(raw_dir, id, f'{session}_raw.fif')\n",
    "        preprocessed_path = os.path.join(preprocessed_dir, id, f'{session}_raw.fif')\n",
    "        bads_path = os.path.join(preprocessed_dir, id, f'{session}_bads.txt')\n",
    "        ica_path = os.path.join(ica_dir, id, f'{session}_{ICA_COMPS}_ica.fif')\n",
    "        ica_drops_path = os.path.join(processed_dir, id, f'{session}_{ICA_COMPS}_ica_drops.txt')\n",
    "        processed_path = os.path.join(processed_dir, id, f'{session}_{ICA_COMPS}_raw.fif')\n",
    "        connectivity_paths = {\n",
    "            \"demo_NoG\": os.path.join(connectivity_dir, id, \"NoG_connectivity.npy\"),\n",
    "            \"demo_WiG\": os.path.join(connectivity_dir, id, \"WiG_connectivity.npy\"),\n",
    "            \"BL_NoG\": os.path.join(connectivity_dir, id, \"BL_NoG_connectivity.npy\"),\n",
    "            \"BL_WiG\": os.path.join(connectivity_dir, id, \"BL_WiG_connectivity.npy\"),\n",
    "        }\n",
    "        note_path = os.path.join(notes_dir, id, 'info.txt')\n",
    "\n",
    "\n",
    "        fdt_set_exists.append(os.path.exists(fdt_path) and os.path.exists(set_path))\n",
    "        raw_exists.append(os.path.exists(raw_path))\n",
    "        preprocessed_exists.append(os.path.exists(preprocessed_path))\n",
    "        \n",
    "        # Read the bads file and append the first line to bads_file_exists\n",
    "        if os.path.exists(bads_path):\n",
    "            with open(bads_path, 'r') as file:\n",
    "                bads_file_exists.append(file.readline())\n",
    "        else:\n",
    "            bads_file_exists.append(None)\n",
    "\n",
    "        ica_exists.append(os.path.exists(ica_path))\n",
    "\n",
    "        # Read the ica drops file and append the first line to ica_drops\n",
    "        if os.path.exists(ica_drops_path):\n",
    "            with open(ica_drops_path, 'r') as file:\n",
    "                ica_drops.append(file.readline())\n",
    "        else:\n",
    "            ica_drops.append(None)\n",
    "\n",
    "        processed_exists.append(os.path.exists(processed_path))\n",
    "\n",
    "        connectivity_for = []\n",
    "        for key in connectivity_paths.keys():\n",
    "            path = connectivity_paths[key]\n",
    "            if(os.path.exists(path)):\n",
    "                connectivity_for.append(key)\n",
    "        if len(connectivity_for) == 4:\n",
    "            connectivity_exists.append(\"All 4\")\n",
    "        else:\n",
    "            connectivity_exists.append(connectivity_for)\n",
    "\n",
    "\n",
    "        # Read note file and append to notes\n",
    "        if os.path.exists(note_path):\n",
    "            with open(note_path, 'r') as file:\n",
    "                notes.append(file.readline())\n",
    "        else:\n",
    "            notes.append(None)\n",
    "\n",
    "\n",
    "\n",
    "    df[\"Channel Locs Added via EEGLAB\"] = fdt_set_exists\n",
    "    df[\"Converted to MNE\"] = raw_exists\n",
    "    df[\"Bads Removed\"] = bads_file_exists\n",
    "    df[\"Preprocessed\"] = preprocessed_exists\n",
    "    df[f'{ICA_COMPS} Comp ICA Generated'] = ica_exists\n",
    "    df[\"ICA Dropped Channels\"] = ica_drops\n",
    "    df[\"Processed\"] = processed_exists\n",
    "    df[\"Connectivity Calculated\"] = connectivity_exists\n",
    "    df[\"Notes\"] = notes\n",
    "\n",
    "    return df\n",
    "\n",
    "def color_code(val):\n",
    "    color = 'green' if val else 'red'\n",
    "    if color == 'red':\n",
    "        color = '#FF9999'\n",
    "    if color == 'green':\n",
    "        color = '#82ffa3'\n",
    "    return f'background-color: {color}'\n",
    "\n",
    "def style(df):\n",
    "    styled_df = df.style.hide(axis='index')\n",
    "    styled_df.set_table_styles([{'selector': 'th', 'props': [('font-size', '12pt')]}])\n",
    "    styled_df.map(color_code, subset=df.columns[2:-1])\n",
    "\n",
    "    # Make text black for all cells except the first two columns\n",
    "    styled_df.set_properties(**{'color': 'black'}, subset=styled_df.columns[2:-1])\n",
    "    \n",
    "    return styled_df\n",
    "\n",
    "def build_dfs():\n",
    "    raw = os.path.join(root, \"raw\")\n",
    "    raw_expert = os.path.join(raw, \"expert\")\n",
    "    raw_novice = os.path.join(raw, \"novice\")\n",
    "\n",
    "    preprocessed = os.path.join(root, \"preprocessed\")\n",
    "    preprocessed_expert = os.path.join(preprocessed, \"expert\")\n",
    "    preprocessed_novice = os.path.join(preprocessed, \"novice\")\n",
    "\n",
    "    processed = os.path.join(root, \"processed\")\n",
    "    processed_expert = os.path.join(processed, \"expert\")\n",
    "    processed_novice = os.path.join(processed, \"novice\")\n",
    "\n",
    "    ica = os.path.join(root, \"ica\")\n",
    "    ica_expert = os.path.join(ica, \"expert\")\n",
    "    ica_novice = os.path.join(ica, \"novice\")\n",
    "\n",
    "    connectivity = os.path.join(root, connectivity_dir)\n",
    "    connectivity_expert = os.path.join(connectivity, \"expert\")\n",
    "    connectivity_novice = os.path.join(connectivity, \"novice\")\n",
    "\n",
    "    notes = os.path.join(root, \"notes\")\n",
    "    notes_expert = os.path.join(notes, \"expert\")\n",
    "    notes_novice = os.path.join(notes, \"novice\")\n",
    "\n",
    "    expert_ids = [d for d in os.listdir(raw_expert) if os.path.isdir(os.path.join(raw_expert, d))]\n",
    "    novice_ids = [d for d in os.listdir(raw_novice) if os.path.isdir(os.path.join(raw_novice, d))]\n",
    "\n",
    "    expert_ids = [str(item) for item in expert_ids for i in range(4)]\n",
    "    novice_ids = [str(item) for item in novice_ids for i in range(4)]\n",
    "\n",
    "    expert_df = pd.DataFrame(expert_ids, columns=[\"id\"])\n",
    "    novice_df = pd.DataFrame(novice_ids, columns=[\"id\"])\n",
    "\n",
    "    expert_df[\"session\"] = expert_df.groupby(\"id\").cumcount() + 1\n",
    "    novice_df[\"session\"] = novice_df.groupby(\"id\").cumcount() + 1\n",
    "\n",
    "    expert_df = process(expert_df, raw_expert, preprocessed_expert, processed_expert, ica_expert, connectivity_expert, notes_expert)\n",
    "    novice_df = process(novice_df, raw_novice, preprocessed_novice, processed_novice, ica_novice, connectivity_novice, notes_novice)\n",
    "\n",
    "\n",
    "    # Hide any row where session == 3\n",
    "    # expert_df = expert_df[expert_df[\"session\"] != 3]\n",
    "    # novice_df = novice_df[novice_df[\"session\"] != 3]\n",
    "\n",
    "    # Drop rows where Channel Locs Added via EEGLAB is False and session >= 3\n",
    "    expert_df = expert_df[~((expert_df[\"Channel Locs Added via EEGLAB\"] == False) & (expert_df[\"session\"] >= 3))]\n",
    "    novice_df = novice_df[~((novice_df[\"Channel Locs Added via EEGLAB\"] == False) & (novice_df[\"session\"] >= 3))]\n",
    "    \n",
    "    # drop expert 5c session 3\n",
    "    # expert_df = expert_df[~((expert_df[\"id\"] == \"5c\") & (expert_df[\"session\"] == 3))]\n",
    "    # # drop expert 5b session 3\n",
    "    # expert_df = expert_df[~((expert_df[\"id\"] == \"5b\") & (expert_df[\"session\"] == 3))]\n",
    "\n",
    "    # Drop any row where Channel Locs Added via EEGLAB is false\n",
    "    expert_df = expert_df[expert_df[\"Channel Locs Added via EEGLAB\"] == True] \n",
    "    novice_df = novice_df[novice_df[\"Channel Locs Added via EEGLAB\"] == True]\n",
    "\n",
    "\n",
    "    # Replace Nones with empty strings in Info Column\n",
    "    expert_df[\"Notes\"] = expert_df[\"Notes\"].fillna('')\n",
    "    novice_df[\"Notes\"] = novice_df[\"Notes\"].fillna('')\n",
    "\n",
    "    \n",
    "    return expert_df, novice_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_df, novice_df = build_dfs()\n",
    "display(style(expert_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_df, novice_df = build_dfs()\n",
    "display(style(novice_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper for saving to csv if needed\n",
    "# cols_of_interest = [\"id\", \"session\", \"Connectivity Calculated\", \"Notes\"]\n",
    "\n",
    "# # Isolate cols of interest\n",
    "# novice_df_summary = novice_df[cols_of_interest]\n",
    "# # Keep only rows where session is 2\n",
    "# novice_df_summary = novice_df_summary[novice_df_summary[\"session\"] == 2]\n",
    "# # Dump to csv\n",
    "# novice_df_summary.to_csv(\"novice_summary.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers for automatically generating bash scripts to run pipelines for missing data\n",
    "# target = \"expert\"\n",
    "# working_df = novice_df if target == \"novice\" else expert_df\n",
    "# expert_drop_ids = [\"13b\", \"11\", \"1\"]\n",
    "# novice_drop_ids = [\"3b\", \"6\", \"13\"]\n",
    "\n",
    "# if target == \"expert\":\n",
    "#     working_df = working_df[~working_df[\"id\"].isin(expert_drop_ids)]\n",
    "\n",
    "# if target == \"novice\":\n",
    "#     working_df = working_df[~working_df[\"id\"].isin(novice_drop_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every column where \"Channel Locs Added via EEGLAB\" is True, but \"Converted to MNE\" is False, run $ python 0_mark_bads.py expert id session\n",
    "# with open(\"run_all_0.sh\", \"w\") as file:\n",
    "#     for _, row in working_df.iterrows():\n",
    "#         if row[\"Channel Locs Added via EEGLAB\"] and not row[\"Converted to MNE\"]:\n",
    "#             print(\"python\", \"0_mark_bads.py\", target, row[\"id\"], str(row[\"session\"]))\n",
    "#             file.write(f'python 0_mark_bads.py {target} {row[\"id\"]} {str(row[\"session\"])}\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every column where \"Channel Locs Added via EEGLAB\" is True, but \"ICA_COMPS Comp ICA Generated\" is False, run $ python 1_preproc.py expert id session\n",
    "# with open(\"run_all_1.sh\", \"w\") as file:\n",
    "#     for _, row in working_df.iterrows():\n",
    "#         if row[\"Channel Locs Added via EEGLAB\"] and not row[f'{ICA_COMPS} Comp ICA Generated']:\n",
    "#             print(\"python\", \"1_preproc.py\", target, row[\"id\"], str(row[\"session\"]), \"--num_ica_comps\", ICA_COMPS)\n",
    "#             file.write(f\"python 1_preproc.py {target} {row['id']} {row['session']} --num_ica_comps {ICA_COMPS}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every column where \"ICA_COMPS Comp ICA Generated\"  is True, but ICA Dropped Channels is False, run $ python 3_compute_connectivity.py expert id session --num_ica_comps ICA_COMPS\n",
    "\n",
    "# with open(\"run_all_2.sh\", \"w\") as file:\n",
    "#     for _, row in working_df.iterrows():\n",
    "#         if row[f'{ICA_COMPS} Comp ICA Generated'] and row[\"ICA Dropped Channels\"] is None:\n",
    "#             print(\"python 2_select_ica.py\", target, row[\"id\"], row[\"session\"], \"--num_ica_comps\", ICA_COMPS)\n",
    "#             file.write(f\"python 2_select_ica.py {target} {row['id']} {row['session']} --num_ica_comps {ICA_COMPS}\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For every column where \"Processed\" is True, but \"Avg Connectivity Calculated\" is None, run $ python 2_select_ica.py expert id session --num_ica_comps ICA_COMPS --filter_string \"BL_NoG\"\n",
    "# ids_added = []\n",
    "# with open(\"run_all_3.sh\", \"w\") as file:\n",
    "#     for _, row in working_df.iterrows():\n",
    "        \n",
    "#         if row[\"Processed\"] and not row[\"Connectivity Calculated\"]:\n",
    "#             # Skip if id already added\n",
    "#             if row[\"id\"] in ids_added:\n",
    "#                 pass\n",
    "#             else:\n",
    "#                 ids_added.append(row[\"id\"])\n",
    "#                 print(\"python\", \"3_compute_connectivity.py\", target, row[\"id\"], \"--num_ica_comps\", ICA_COMPS, \"--baseline True --WiG False\")\n",
    "#                 print(\"python\", \"3_compute_connectivity.py\", target, row[\"id\"], \"--num_ica_comps\", ICA_COMPS, \"--baseline False --WiG False\")\n",
    "#                 print(\"python\", \"3_compute_connectivity.py\", target, row[\"id\"], \"--num_ica_comps\", ICA_COMPS, \"--baseline True --WiG True\")\n",
    "#                 print(\"python\", \"3_compute_connectivity.py\", target, row[\"id\"], \"--num_ica_comps\", ICA_COMPS, \"--baseline False --WiG True\")\n",
    "                \n",
    "#                 file.write(f\"python 3_compute_connectivity.py {target} {row['id']} --num_ica_comps {ICA_COMPS} --baseline True --WiG False\\n\")\n",
    "#                 file.write(f\"python 3_compute_connectivity.py {target} {row['id']} --num_ica_comps {ICA_COMPS} --baseline False --WiG False\\n\")\n",
    "#                 file.write(f\"python 3_compute_connectivity.py {target} {row['id']} --num_ica_comps {ICA_COMPS} --baseline True --WiG True\\n\")\n",
    "#                 file.write(f\"python 3_compute_connectivity.py {target} {row['id']} --num_ica_comps {ICA_COMPS} --baseline False --WiG True\\n\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
